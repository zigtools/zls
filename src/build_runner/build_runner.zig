//! PLEASE READ THE FOLLOWING MESSAGE BEFORE EDITING THIS FILE:
//!
//! To support the Zig build system, ZLS uses a custom build runner which
//! interfaces with the compiler that is installed on the user's system.
//!
//! The Zig version on the user's system may not be the same version that ZLS
//! has been built with. The minimum runtime Zig version in the `build.zig`
//! specifies which Zig versions need to be supported by this build runner.
//!
//! When a breaking change occurs, this is what need to be done:
//!   - If it is a tagged release      (e.g 0.x.y)    , update the build runner code to handle multiple Zig versions.
//!   - If it is a development version (e.g 0.x.y-dev), bump the minimum runtime Zig version.
//!
//! Handling multiple Zig versions can be achieved with one of the following strategies:
//!   - use `@hasDecl` or `@hasField` (recommended)
//!   - use `builtin.zig_version`
//!
//! The build runner can be tested on ZLS's `build.zig` with the following command:
//! `zig build --build-runner src/build_runner/build_runner.zig`
//!
//! To test with a different `build.zig`, use one of the following commands:
//! `zig build --build-file /path/to/build.zig --build-runner /path/to/zls/src/build_runner/build_runner.zig`
//! `zig build --build-runner /path/to/zls/src/build_runner/build_runner.zig` (if the cwd contains build.zig)
//!

const std = @import("std");
const builtin = @import("builtin");
const Io = std.Io;
const assert = std.debug.assert;
const mem = std.mem;
const process = std.process;
const File = Io.File;
const Step = std.Build.Step;
const Allocator = std.mem.Allocator;
const fatal = std.process.fatal;
const runner = @This();

pub const root = @import("@build");
pub const dependencies = @import("@dependencies");

pub const std_options: std.Options = .{
    .side_channels_mitigations = .none,
    .http_disable_tls = true,
};

pub fn main(init: process.Init.Minimal) !void {
    // The build runner is often short-lived, but thanks to `--watch` and `--webui`, that's not
    // always the case. So, we do need a true gpa for some things.
    var debug_gpa_state: std.heap.DebugAllocator(.{}) = .init;
    defer _ = debug_gpa_state.deinit();
    const gpa = debug_gpa_state.allocator();

    // ...but we'll back our arena by `std.heap.page_allocator` for efficiency.
    var single_threaded_arena: std.heap.ArenaAllocator = .init(std.heap.page_allocator);
    defer single_threaded_arena.deinit();
    var thread_safe_arena: std.heap.ThreadSafeAllocator = .{ .child_allocator = single_threaded_arena.allocator() };
    const arena = thread_safe_arena.allocator();

    const args = try init.args.toSlice(arena);

    var threaded: Io.Threaded = .init(gpa, .{
        .environ = init.environ,
        .argv0 = .init(init.args),
    });
    defer threaded.deinit();
    const io = threaded.ioBasic();

    // skip my own exe name
    var arg_idx: usize = 1;

    const zig_exe = nextArg(args, &arg_idx) orelse fatal("missing zig compiler path", .{});
    const zig_lib_dir = nextArg(args, &arg_idx) orelse fatal("missing zig lib directory path", .{});
    const build_root = nextArg(args, &arg_idx) orelse fatal("missing build root directory path", .{});
    const cache_root = nextArg(args, &arg_idx) orelse fatal("missing cache root directory path", .{});
    const global_cache_root = nextArg(args, &arg_idx) orelse fatal("missing global cache root directory path", .{});

    const cwd: Io.Dir = .cwd();

    const zig_lib_directory: std.Build.Cache.Directory = .{
        .path = zig_lib_dir,
        .handle = try cwd.openDir(io, zig_lib_dir, .{}),
    };

    const build_root_directory: std.Build.Cache.Directory = .{
        .path = build_root,
        .handle = try cwd.openDir(io, build_root, .{}),
    };

    const local_cache_directory: std.Build.Cache.Directory = .{
        .path = cache_root,
        .handle = try cwd.createDirPathOpen(io, cache_root, .{}),
    };

    const global_cache_directory: std.Build.Cache.Directory = .{
        .path = global_cache_root,
        .handle = try cwd.createDirPathOpen(io, global_cache_root, .{}),
    };

    var graph: std.Build.Graph = .{
        .io = io,
        .arena = arena,
        .cache = .{
            .io = io,
            .gpa = gpa,
            .manifest_dir = try local_cache_directory.handle.createDirPathOpen(io, "h", .{}),
            .cwd = try process.currentPathAlloc(io, single_threaded_arena.allocator()),
        },
        .zig_exe = zig_exe,
        .environ_map = try init.environ.createMap(arena),
        .global_cache_root = global_cache_directory,
        .zig_lib_directory = zig_lib_directory,
        .host = .{
            .query = .{},
            .result = try std.zig.system.resolveTargetQuery(io, .{}),
        },
        .time_report = false,
    };

    graph.cache.addPrefix(.{ .path = null, .handle = cwd });
    graph.cache.addPrefix(build_root_directory);
    graph.cache.addPrefix(local_cache_directory);
    graph.cache.addPrefix(global_cache_directory);
    graph.cache.hash.addBytes(builtin.zig_version_string);

    const builder = try std.Build.create(
        &graph,
        build_root_directory,
        local_cache_directory,
        dependencies.root_deps,
    );

    var targets: std.ArrayList([]const u8) = .empty;

    var install_prefix: ?[]const u8 = null;
    var dir_list: std.Build.DirList = .{};
    var max_rss: u64 = 0;
    var skip_oom_steps = false;
    var output_tmp_nonce: ?[16]u8 = null;
    var watch = false;
    var check_step_only = false;
    var debounce_interval_ms: u16 = 50;

    while (nextArg(args, &arg_idx)) |arg| {
        if (mem.startsWith(u8, arg, "-Z")) {
            if (arg.len != 18) fatalWithHint("bad argument: '{s}'", .{arg});
            output_tmp_nonce = arg[2..18].*;
        } else if (mem.startsWith(u8, arg, "-D")) {
            const option_contents = arg[2..];
            if (option_contents.len == 0)
                fatalWithHint("expected option name after '-D'", .{});
            if (mem.indexOfScalar(u8, option_contents, '=')) |name_end| {
                const option_name = option_contents[0..name_end];
                const option_value = option_contents[name_end + 1 ..];
                if (try builder.addUserInputOption(option_name, option_value))
                    fatal("  access the help menu with 'zig build -h'", .{});
            } else {
                if (try builder.addUserInputFlag(option_contents))
                    fatal("  access the help menu with 'zig build -h'", .{});
            }
        } else if (mem.startsWith(u8, arg, "-")) {
            if (mem.eql(u8, arg, "--verbose")) {
                builder.verbose = true;
            } else if (mem.eql(u8, arg, "-h") or mem.eql(u8, arg, "--help")) {
                fatal("argument '{s}' is not available", .{arg});
            } else if (mem.eql(u8, arg, "-p") or mem.eql(u8, arg, "--prefix")) {
                install_prefix = nextArgOrFatal(args, &arg_idx);
            } else if (mem.eql(u8, arg, "-l") or mem.eql(u8, arg, "--list-steps")) {
                fatal("argument '{s}' is not available", .{arg});
            } else if (mem.startsWith(u8, arg, "-fsys=")) {
                const name = arg["-fsys=".len..];
                graph.system_library_options.put(arena, name, .user_enabled) catch @panic("OOM");
            } else if (mem.startsWith(u8, arg, "-fno-sys=")) {
                const name = arg["-fno-sys=".len..];
                graph.system_library_options.put(arena, name, .user_disabled) catch @panic("OOM");
            } else if (mem.eql(u8, arg, "--release")) {
                builder.release_mode = .any;
            } else if (mem.startsWith(u8, arg, "--release=")) {
                const text = arg["--release=".len..];
                builder.release_mode = std.meta.stringToEnum(std.Build.ReleaseMode, text) orelse {
                    fatalWithHint("expected [off|any|fast|safe|small] in '{s}', found '{s}'", .{
                        arg, text,
                    });
                };
            } else if (mem.eql(u8, arg, "--prefix-lib-dir")) {
                dir_list.lib_dir = nextArgOrFatal(args, &arg_idx);
            } else if (mem.eql(u8, arg, "--prefix-exe-dir")) {
                dir_list.exe_dir = nextArgOrFatal(args, &arg_idx);
            } else if (mem.eql(u8, arg, "--prefix-include-dir")) {
                dir_list.include_dir = nextArgOrFatal(args, &arg_idx);
            } else if (mem.eql(u8, arg, "--sysroot")) {
                builder.sysroot = nextArgOrFatal(args, &arg_idx);
            } else if (mem.eql(u8, arg, "--maxrss")) {
                const max_rss_text = nextArgOrFatal(args, &arg_idx);
                max_rss = std.fmt.parseIntSizeSuffix(max_rss_text, 10) catch |err| {
                    std.debug.print("invalid byte size: '{s}': {s}\n", .{
                        max_rss_text, @errorName(err),
                    });
                    process.exit(1);
                };
            } else if (mem.eql(u8, arg, "--skip-oom-steps")) {
                skip_oom_steps = true;
            } else if (mem.eql(u8, arg, "--test-timeout")) {
                const next_arg = nextArgOrFatal(args, &arg_idx);
                _ = next_arg;
            } else if (mem.eql(u8, arg, "--search-prefix")) {
                const search_prefix = nextArgOrFatal(args, &arg_idx);
                builder.addSearchPrefix(search_prefix);
            } else if (mem.eql(u8, arg, "--libc")) {
                builder.libc_file = nextArgOrFatal(args, &arg_idx);
            } else if (mem.eql(u8, arg, "--color")) {
                const next_arg = nextArg(args, &arg_idx) orelse
                    fatalWithHint("expected [auto|on|off] after '{s}'", .{arg});
                _ = next_arg;
            } else if (mem.eql(u8, arg, "--error-style")) {
                const next_arg = nextArg(args, &arg_idx) orelse
                    fatalWithHint("expected style after '{s}'", .{arg});
                _ = next_arg;
            } else if (mem.eql(u8, arg, "--multiline-errors")) {
                const next_arg = nextArg(args, &arg_idx) orelse
                    fatalWithHint("expected style after '{s}'", .{arg});
                _ = next_arg;
            } else if (mem.eql(u8, arg, "--summary")) {
                const next_arg = nextArg(args, &arg_idx) orelse
                    fatalWithHint("expected [all|new|failures|line|none] after '{s}'", .{arg});
                _ = next_arg;
            } else if (mem.eql(u8, arg, "--seed")) {
                const next_arg = nextArg(args, &arg_idx) orelse
                    fatalWithHint("expected u32 after '{s}'", .{arg});
                graph.random_seed = std.fmt.parseUnsigned(u32, next_arg, 0) catch |err| {
                    fatal("unable to parse seed '{s}' as unsigned 32-bit integer: {s}\n", .{
                        next_arg, @errorName(err),
                    });
                };
            } else if (mem.eql(u8, arg, "--build-id")) {
                builder.build_id = .fast;
            } else if (mem.startsWith(u8, arg, "--build-id=")) {
                const style = arg["--build-id=".len..];
                builder.build_id = std.zig.BuildId.parse(style) catch |err| {
                    fatal("unable to parse --build-id style '{s}': {s}", .{
                        style, @errorName(err),
                    });
                };
            } else if (mem.eql(u8, arg, "--debounce")) {
                const next_arg = nextArg(args, &arg_idx) orelse
                    fatalWithHint("expected u16 after '{s}'", .{arg});
                debounce_interval_ms = std.fmt.parseUnsigned(u16, next_arg, 0) catch |err| {
                    fatal("unable to parse debounce interval '{s}' as unsigned 16-bit integer: {t}\n", .{
                        next_arg, err,
                    });
                };
            } else if (mem.eql(u8, arg, "--webui")) {
                fatal("argument '{s}' is not available", .{arg});
            } else if (mem.startsWith(u8, arg, "--webui=")) {
                fatal("argument '{s}' is not available", .{arg});
            } else if (mem.eql(u8, arg, "--debug-log")) {
                fatal("argument '{s}' is not available", .{arg});
            } else if (mem.eql(u8, arg, "--debug-pkg-config")) {
                builder.debug_pkg_config = true;
            } else if (mem.eql(u8, arg, "--debug-rt")) {
                graph.debug_compiler_runtime_libs = true;
            } else if (mem.eql(u8, arg, "--debug-compile-errors")) {
                builder.debug_compile_errors = true;
            } else if (mem.eql(u8, arg, "--debug-incremental")) {
                builder.debug_incremental = true;
            } else if (mem.eql(u8, arg, "--system")) {
                // The usage text shows another argument after this parameter
                // but it is handled by the parent process. The build runner
                // only sees this flag.
                graph.system_package_mode = true;
            } else if (mem.eql(u8, arg, "--libc-runtimes") or mem.eql(u8, arg, "--glibc-runtimes")) {
                // --glibc-runtimes was the old name of the flag; kept for compatibility for now.
                builder.libc_runtimes_dir = nextArgOrFatal(args, &arg_idx);
            } else if (mem.eql(u8, arg, "--verbose-link")) {
                builder.verbose_link = true;
            } else if (mem.eql(u8, arg, "--verbose-air")) {
                builder.verbose_air = true;
            } else if (mem.eql(u8, arg, "--verbose-llvm-ir")) {
                builder.verbose_llvm_ir = "-";
            } else if (mem.startsWith(u8, arg, "--verbose-llvm-ir=")) {
                builder.verbose_llvm_ir = arg["--verbose-llvm-ir=".len..];
            } else if (mem.startsWith(u8, arg, "--verbose-llvm-bc=")) {
                builder.verbose_llvm_bc = arg["--verbose-llvm-bc=".len..];
            } else if (mem.eql(u8, arg, "--verbose-cimport")) {
                builder.verbose_cimport = true;
            } else if (mem.eql(u8, arg, "--verbose-cc")) {
                builder.verbose_cc = true;
            } else if (mem.eql(u8, arg, "--verbose-llvm-cpu-features")) {
                builder.verbose_llvm_cpu_features = true;
            } else if (mem.eql(u8, arg, "--watch")) {
                watch = true;
            } else if (mem.eql(u8, arg, "--check-only")) { // ZLS only
                check_step_only = true;
            } else if (mem.eql(u8, arg, "--time-report")) {
                fatal("argument '{s}' is not available", .{arg});
            } else if (mem.startsWith(u8, arg, "--fuzz")) {
                fatal("argument '{s}' is not available", .{arg});
            } else if (mem.eql(u8, arg, "-fincremental")) {
                graph.incremental = true;
            } else if (mem.eql(u8, arg, "-fno-incremental")) {
                graph.incremental = false;
            } else if (mem.eql(u8, arg, "-fwine")) {
                builder.enable_wine = true;
            } else if (mem.eql(u8, arg, "-fno-wine")) {
                builder.enable_wine = false;
            } else if (mem.eql(u8, arg, "-fqemu")) {
                builder.enable_qemu = true;
            } else if (mem.eql(u8, arg, "-fno-qemu")) {
                builder.enable_qemu = false;
            } else if (mem.eql(u8, arg, "-fwasmtime")) {
                builder.enable_wasmtime = true;
            } else if (mem.eql(u8, arg, "-fno-wasmtime")) {
                builder.enable_wasmtime = false;
            } else if (mem.eql(u8, arg, "-frosetta")) {
                builder.enable_rosetta = true;
            } else if (mem.eql(u8, arg, "-fno-rosetta")) {
                builder.enable_rosetta = false;
            } else if (mem.eql(u8, arg, "-fdarling")) {
                builder.enable_darling = true;
            } else if (mem.eql(u8, arg, "-fno-darling")) {
                builder.enable_darling = false;
            } else if (mem.eql(u8, arg, "-fallow-so-scripts")) {
                graph.allow_so_scripts = true;
            } else if (mem.eql(u8, arg, "-fno-allow-so-scripts")) {
                graph.allow_so_scripts = false;
            } else if (mem.eql(u8, arg, "-freference-trace")) {
                builder.reference_trace = 256;
            } else if (mem.startsWith(u8, arg, "-freference-trace=")) {
                const num = arg["-freference-trace=".len..];
                builder.reference_trace = std.fmt.parseUnsigned(u32, num, 10) catch |err| {
                    std.debug.print("unable to parse reference_trace count '{s}': {s}", .{ num, @errorName(err) });
                    process.exit(1);
                };
            } else if (mem.eql(u8, arg, "-fno-reference-trace")) {
                builder.reference_trace = null;
            } else if (mem.cutPrefix(u8, arg, "-j")) |text| {
                const n = std.fmt.parseUnsigned(u32, text, 10) catch |err|
                    fatal("unable to parse jobs count '{s}': {t}", .{ text, err });
                if (n < 1) fatal("number of jobs must be at least 1", .{});
                threaded.setAsyncLimit(.limited(n));
            } else if (mem.eql(u8, arg, "--")) {
                builder.args = argsRest(args, arg_idx);
                break;
            } else {
                fatalWithHint("unrecognized argument: '{s}'", .{arg});
            }
        } else {
            try targets.append(arena, arg);
        }
    }

    const main_progress_node = std.Progress.start(io, .{
        .disable_printing = true,
    });
    defer main_progress_node.end();

    builder.resolveInstallPrefix(install_prefix, dir_list);
    {
        var prog_node = main_progress_node.start("Configure", 0);
        defer prog_node.end();
        try builder.runBuild(root);
        createModuleDependencies(builder) catch @panic("OOM");
    }

    if (graph.needed_lazy_dependencies.entries.len != 0) {
        var buffer: std.ArrayList(u8) = .empty;
        for (graph.needed_lazy_dependencies.keys()) |k| {
            try buffer.appendSlice(arena, k);
            try buffer.append(arena, '\n');
        }
        const s = std.fs.path.sep_str;
        const tmp_sub_path = "tmp" ++ s ++ (output_tmp_nonce orelse fatal("missing -Z arg", .{}));
        local_cache_directory.handle.writeFile(io, .{
            .sub_path = tmp_sub_path,
            .data = buffer.items,
            .flags = .{ .exclusive = true },
        }) catch |err| {
            fatal("unable to write configuration results to '{f}{s}': {s}", .{
                local_cache_directory, tmp_sub_path, @errorName(err),
            });
        };
        process.exit(3); // Indicate configure phase failed with meaningful stdout.
    }

    if (builder.validateUserInputDidItFail()) {
        fatal("  access the help menu with 'zig build -h'", .{});
    }

    validateSystemLibraryOptions(builder);

    var run: Run = .{
        .gpa = gpa,

        .available_rss = max_rss,
        .max_rss_is_default = false,
        .max_rss_mutex = .init,
        .skip_oom_steps = skip_oom_steps,
        .watch = watch,
        .memory_blocked_steps = .empty,

        .cycle = 0,
    };
    defer run.memory_blocked_steps.deinit(gpa);

    if (run.available_rss == 0) {
        run.available_rss = process.totalSystemMemory() catch std.math.maxInt(u64);
        run.max_rss_is_default = true;
    }

    if (!watch) {
        try extractBuildInformation(
            builder,
            arena,
            main_progress_node,
            &run,
        );
        std.process.exit(0);
    }

    var w = try Watch.init(io, graph.cache.cwd);

    const message_thread = try std.Thread.spawn(.{}, struct {
        fn do(ww: *Watch) void {
            while (true) {
                var buffer: [1]u8 = undefined;
                var stdin_reader = Io.File.stdin().reader(ww.io, &buffer);
                const byte = stdin_reader.interface.takeByte() catch |err| switch (err) {
                    error.ReadFailed => process.exit(1),
                    error.EndOfStream => process.exit(0),
                };
                switch (byte) {
                    '\x00' => ww.trigger(),
                    else => process.exit(1),
                }
            }
        }
    }.do, .{&w});
    message_thread.detach();

    var step_stack = try resolveStepNames(gpa, builder, targets.items, check_step_only);
    defer step_stack.deinit(gpa);
    if (step_stack.count() == 0) {
        // This means that `enable_build_on_save == null` and the project contains no "check" step.
        return;
    }

    const starting_steps = try gpa.dupe(*Step, step_stack.keys());
    defer gpa.free(starting_steps);

    prepare(builder, &step_stack, &run) catch |err| switch (err) {
        error.DependencyLoopDetected => process.exit(1),
        else => |e| return e,
    };

    rebuild: while (true) : (run.cycle += 1) {
        try runSteps(
            builder,
            &step_stack,
            main_progress_node,
            &run,
        );

        try w.update(gpa, step_stack.keys());

        // Wait until a file system notification arrives. Read all such events
        // until the buffer is empty. Then wait for a debounce interval, resetting
        // if any more events come in. After the debounce interval has passed,
        // trigger a rebuild on all steps with modified inputs, as well as their
        // recursive dependants.
        var debounce_timeout: Io.Timeout = .none;
        while (true) switch (try w.wait(gpa, debounce_timeout)) {
            .timeout => {
                markFailedStepsDirty(gpa, step_stack.keys());
                continue :rebuild;
            },
            .dirty => if (debounce_timeout == .none) {
                debounce_timeout = .{ .duration = .{ .raw = .fromMilliseconds(debounce_interval_ms), .clock = .real } };
            },
            .clean => {},
        };
    }
}

fn markFailedStepsDirty(gpa: Allocator, all_steps: []const *Step) void {
    for (all_steps) |step| switch (step.state) {
        .dependency_failure, .failure, .skipped => _ = step.invalidateResult(gpa),
        else => continue,
    };
    // Now that all dirty steps have been found, the remaining steps that
    // succeeded from last run shall be marked "cached".
    for (all_steps) |step| switch (step.state) {
        .success => step.result_cached = true,
        else => continue,
    };
}

/// A wrapper around `std.Build.Watch` that supports manually triggering recompilations.
const Watch = struct {
    io: std.Io,
    fs_watch: std.Build.Watch,
    supports_fs_watch: bool,
    manual_event: Io.Event,
    steps: []const *Step,

    fn init(io: std.Io, cwd_path: []const u8) !Watch {
        return .{
            .io = io,
            .fs_watch = if (@TypeOf(std.Build.Watch) != void) try std.Build.Watch.init(cwd_path) else {},
            .supports_fs_watch = @TypeOf(std.Build.Watch) != void and shared.BuildOnSaveSupport.isSupportedRuntime(builtin.zig_version) == .supported,
            .manual_event = .unset,
            .steps = &.{},
        };
    }

    fn update(w: *Watch, gpa: Allocator, steps: []const *Step) !void {
        if (@TypeOf(std.Build.Watch) != void and w.supports_fs_watch) {
            return try w.fs_watch.update(gpa, steps);
        }
        w.steps = steps;
    }

    fn trigger(w: *Watch) void {
        if (w.supports_fs_watch) {
            @panic("received manualy filesystem event even though std.Build.Watch is supported");
        }
        w.manual_event.set(w.io);
    }

    fn wait(w: *Watch, gpa: Allocator, timeout: Io.Timeout) !std.Build.Watch.WaitResult {
        if (@TypeOf(std.Build.Watch) != void and w.supports_fs_watch) {
            return try w.fs_watch.wait(gpa, switch (timeout) {
                .none => .none,
                .duration => |d| .{ .ms = @intCast(d.raw.toMilliseconds()) },
                .deadline => unreachable,
            });
        }
        waitTimeout(&w.manual_event, w.io, timeout) catch |err| switch (err) {
            error.Canceled => unreachable,
            error.Timeout => return .timeout,
        };
        w.manual_event.reset();
        markStepsDirty(gpa, w.steps);
        return .dirty;
    }

    /// Copy of `Io.Event.waitTimeout` but a compile error has been fixed.
    pub fn waitTimeout(event: *Io.Event, io: std.Io, timeout: Io.Timeout) (error{Timeout} || Io.Cancelable)!void {
        if (@cmpxchgStrong(Io.Event, event, .unset, .waiting, .acquire, .acquire)) |prev| switch (prev) {
            .unset => unreachable,
            .waiting => assert(!builtin.single_threaded), // invalid state
            .is_set => return,
        };
        try io.futexWaitTimeout(Io.Event, event, .waiting, timeout);
        switch (@atomicLoad(Io.Event, event, .acquire)) {
            .unset => unreachable, // `reset` called before pending `wait` returned
            .waiting => return error.Timeout,
            .is_set => return,
        }
    }

    fn markStepsDirty(gpa: Allocator, all_steps: []const *Step) void {
        for (all_steps) |step| switch (step.state) {
            .precheck_done => continue,
            else => _ = step.invalidateResult(gpa),
        };
    }
};

const Run = struct {
    gpa: Allocator,

    available_rss: usize,
    max_rss_is_default: bool,
    max_rss_mutex: Io.Mutex,
    skip_oom_steps: bool,
    watch: bool,
    /// Allocated into `gpa`.
    memory_blocked_steps: std.ArrayList(*Step),

    cycle: u32,
};

fn resolveStepNames(
    gpa: Allocator,
    b: *std.Build,
    step_names: []const []const u8,
    check_step_only: bool,
) !std.AutoArrayHashMapUnmanaged(*Step, void) {
    var starting_steps: std.AutoArrayHashMapUnmanaged(*Step, void) = .{};
    errdefer starting_steps.deinit(gpa);

    if (step_names.len == 0) {
        if (b.top_level_steps.get("check")) |tls| {
            try starting_steps.put(gpa, &tls.step, {});
        } else if (!check_step_only) {
            try starting_steps.put(gpa, b.default_step, {});
        }
    } else {
        try starting_steps.ensureUnusedCapacity(gpa, step_names.len);
        for (0..step_names.len) |i| {
            const step_name = step_names[step_names.len - i - 1];
            const s = b.top_level_steps.get(step_name) orelse {
                std.debug.print("no step named '{s}'\n  access the help menu with 'zig build -h'\n", .{step_name});
                process.exit(1);
            };
            starting_steps.putAssumeCapacity(&s.step, {});
        }
    }

    return starting_steps;
}

fn prepare(
    b: *std.Build,
    unpopulated_step_stack: *std.AutoArrayHashMapUnmanaged(*Step, void),
    run: *Run,
) error{ OutOfMemory, DependencyLoopDetected }!void {
    const gpa = run.gpa;

    const starting_steps = try gpa.dupe(*Step, unpopulated_step_stack.keys());
    defer gpa.free(starting_steps);

    var rng = std.Random.DefaultPrng.init(b.graph.random_seed);
    const rand = rng.random();
    rand.shuffle(*Step, starting_steps);

    for (starting_steps) |s| {
        try constructGraphAndCheckForDependencyLoop(gpa, b, s, unpopulated_step_stack, rand);
    }
    const step_stack = unpopulated_step_stack;

    {
        // Check that we have enough memory to complete the build.
        var any_problems = false;
        for (step_stack.keys()) |s| {
            if (s.max_rss == 0) continue;
            if (s.max_rss > run.available_rss) {
                if (run.skip_oom_steps) {
                    s.state = .skipped_oom;
                    for (s.dependants.items) |dependant| {
                        dependant.pending_deps -= 1;
                    }
                } else {
                    std.debug.print("{s}{s}: this step declares an upper bound of {d} bytes of memory, exceeding the available {d} bytes of memory\n", .{
                        s.owner.dep_prefix, s.name, s.max_rss, run.available_rss,
                    });
                    any_problems = true;
                }
            }
        }
        if (any_problems) {
            if (run.max_rss_is_default) {
                std.debug.print("note: use --maxrss to override the default", .{});
            }
        }
    }
}

fn runSteps(
    b: *std.Build,
    step_stack: *const std.AutoArrayHashMapUnmanaged(*Step, void),
    parent_prog_node: std.Progress.Node,
    run: *Run,
) (Io.Cancelable || mem.Allocator.Error)!void {
    const gpa = run.gpa;
    const io = b.graph.io;

    // Collect the initial set of tasks (those with no outstanding dependencies) into a buffer,
    // then spawn them. The buffer is so that we don't race with `makeStep` and end up thinking
    // a step is initial when it actually became ready due to an earlier initial step.
    var initial_set: std.ArrayList(*Step) = .empty;
    defer initial_set.deinit(gpa);
    try initial_set.ensureUnusedCapacity(gpa, step_stack.count());
    for (step_stack.keys()) |s| {
        if (s.state == .precheck_done and s.pending_deps == 0) {
            initial_set.appendAssumeCapacity(s);
        }
    }

    const step_prog = parent_prog_node.start("steps", step_stack.count());
    defer step_prog.end();

    var group: Io.Group = .init;
    defer group.cancel(io);
    // Start working on all of the initial steps...
    for (initial_set.items) |s| try stepReady(&group, b, step_stack, s, step_prog, run);
    // ...and `makeStep` will trigger every other step when their last dependency finishes.
    try group.await(io);
}

/// Traverse the dependency graph depth-first and make it undirected by having
/// steps know their dependants (they only know dependencies at start).
/// Along the way, check that there is no dependency loop, and record the steps
/// in traversal order in `step_stack`.
/// Each step has its dependencies traversed in random order, this accomplishes
/// two things:
/// - `step_stack` will be in randomized-depth-first order, so the build runner
///   spawns initial steps in a random order
/// - each step's `dependants` list is also filled in a random order, so that
///   when it finishes executing in `makeStep`, it spawns next steps to run in
///   random order
fn constructGraphAndCheckForDependencyLoop(
    gpa: Allocator,
    b: *std.Build,
    s: *Step,
    step_stack: *std.AutoArrayHashMapUnmanaged(*Step, void),
    rand: std.Random,
) !void {
    switch (s.state) {
        .precheck_started => {
            return error.DependencyLoopDetected;
        },
        .precheck_unstarted => {
            s.state = .precheck_started;

            try step_stack.ensureUnusedCapacity(gpa, s.dependencies.items.len);

            // We dupe to avoid shuffling the steps in the summary, it depends
            // on s.dependencies' order.
            const deps = gpa.dupe(*Step, s.dependencies.items) catch @panic("OOM");
            defer gpa.free(deps);

            rand.shuffle(*Step, deps);

            for (deps) |dep| {
                try step_stack.put(gpa, dep, {});
                try dep.dependants.append(b.allocator, s);
                constructGraphAndCheckForDependencyLoop(gpa, b, dep, step_stack, rand) catch |err| {
                    return err;
                };
            }

            s.state = .precheck_done;
            s.pending_deps = @intCast(s.dependencies.items.len);
        },
        .precheck_done => {},

        // These don't happen until we actually run the step graph.
        .dependency_failure => unreachable,
        .success => unreachable,
        .failure => unreachable,
        .skipped => unreachable,
        .skipped_oom => unreachable,
    }
}

/// Runs the "make" function of the single step `s`, updates its state, and then spawns newly-ready
/// dependant steps in `group`. If `s` makes an RSS claim (i.e. `s.max_rss != 0`), the caller must
/// have already subtracted this value from `run.available_rss`. This function will release the RSS
/// claim (i.e. add `s.max_rss` back into `run.available_rss`) and queue any viable memory-blocked
/// steps after "make" completes for `s`.
fn makeStep(
    group: *Io.Group,
    b: *std.Build,
    steps_stack: *const std.AutoArrayHashMapUnmanaged(*Step, void),
    s: *Step,
    root_prog_node: std.Progress.Node,
    run: *Run,
) Io.Cancelable!void {
    const io = b.graph.io;
    const gpa = run.gpa;

    {
        const step_prog_node = root_prog_node.start(s.name, 0);
        defer step_prog_node.end();

        const new_state: Step.State = for (s.dependencies.items) |dep| {
            switch (@atomicLoad(Step.State, &dep.state, .monotonic)) {
                .precheck_unstarted => unreachable,
                .precheck_started => unreachable,
                .precheck_done => unreachable,

                .failure,
                .dependency_failure,
                .skipped_oom,
                => break .dependency_failure,

                .success, .skipped => {},
            }
        } else if (s.make(.{
            .progress_node = step_prog_node,
            .watch = run.watch,
            .web_server = null,
            .unit_test_timeout_ns = null,
            .gpa = gpa,
        })) state: {
            break :state .success;
        } else |err| switch (err) {
            error.MakeFailed => .failure,
            error.MakeSkipped => .skipped,
        };

        @atomicStore(Step.State, &s.state, new_state, .monotonic);

        switch (new_state) {
            .precheck_unstarted => unreachable,
            .precheck_started => unreachable,
            .precheck_done => unreachable,

            .failure,
            .dependency_failure,
            .skipped_oom,
            => {
                std.Progress.setStatus(.failure_working);
            },

            .success,
            .skipped,
            => {},
        }
    }

    if (run.watch) {
        const step_id: u32 = @intCast(steps_stack.getIndex(s).?);
        // missing fields:
        // - result_error_msgs
        // - result_stderr
        serveWatchErrorBundle(b.graph.io, step_id, run.cycle, s.result_error_bundle) catch @panic("failed to send watch errors");
    }

    if (s.max_rss != 0) {
        var dispatch_set: std.ArrayList(*Step) = .empty;
        defer dispatch_set.deinit(gpa);

        // Release our RSS claim and kick off some blocked steps if possible. We use `dispatch_set`
        // as a staging buffer to avoid recursing into `makeStep` while `run.max_rss_mutex` is held.
        {
            try run.max_rss_mutex.lock(io);
            defer run.max_rss_mutex.unlock(io);
            run.available_rss += s.max_rss;
            dispatch_set.ensureUnusedCapacity(gpa, run.memory_blocked_steps.items.len) catch @panic("OOM");
            while (run.memory_blocked_steps.getLastOrNull()) |candidate| {
                if (run.available_rss < candidate.max_rss) break;
                assert(run.memory_blocked_steps.pop() == candidate);
                dispatch_set.appendAssumeCapacity(candidate);
            }
        }
        for (dispatch_set.items) |candidate| {
            group.async(io, makeStep, .{ group, b, steps_stack, candidate, root_prog_node, run });
        }
    }

    for (s.dependants.items) |dependant| {
        // `.acq_rel` synchronizes with itself to ensure all dependencies' final states are visible when this hits 0.
        if (@atomicRmw(u32, &dependant.pending_deps, .Sub, 1, .acq_rel) == 1) {
            try stepReady(group, b, steps_stack, dependant, root_prog_node, run);
        }
    }
}

fn stepReady(
    group: *Io.Group,
    b: *std.Build,
    steps_stack: *const std.AutoArrayHashMapUnmanaged(*Step, void),
    s: *Step,
    root_prog_node: std.Progress.Node,
    run: *Run,
) !void {
    const io = b.graph.io;
    if (s.max_rss != 0) {
        try run.max_rss_mutex.lock(io);
        defer run.max_rss_mutex.unlock(io);
        if (run.available_rss < s.max_rss) {
            // Running this step right now could possibly exceed the allotted RSS.
            run.memory_blocked_steps.append(run.gpa, s) catch @panic("OOM");
            return;
        }
        run.available_rss -= s.max_rss;
    }
    group.async(io, makeStep, .{ group, b, steps_stack, s, root_prog_node, run });
}

fn nextArg(args: []const [:0]const u8, idx: *usize) ?[:0]const u8 {
    if (idx.* >= args.len) return null;
    defer idx.* += 1;
    return args[idx.*];
}

fn nextArgOrFatal(args: []const [:0]const u8, idx: *usize) [:0]const u8 {
    return nextArg(args, idx) orelse {
        std.debug.print("expected argument after '{s}'\n  access the help menu with 'zig build -h'\n", .{args[idx.* - 1]});
        process.exit(1);
    };
}

fn argsRest(args: []const [:0]const u8, idx: usize) ?[]const [:0]const u8 {
    if (idx >= args.len) return null;
    return args[idx..];
}

fn fatalWithHint(comptime f: []const u8, args: anytype) noreturn {
    std.debug.print(f ++ "\n  access the help menu with 'zig build -h'\n", args);
    process.exit(1);
}

fn validateSystemLibraryOptions(b: *std.Build) void {
    var bad = false;
    for (b.graph.system_library_options.keys(), b.graph.system_library_options.values()) |k, v| {
        switch (v) {
            .user_disabled, .user_enabled => {
                // The user tried to enable or disable a system library integration, but
                // the build script did not recognize that option.
                std.debug.print("system library name not recognized by build script: '{s}'\n", .{k});
                bad = true;
            },
            .declared_disabled, .declared_enabled => {},
        }
    }
    if (bad) {
        std.debug.print("  access the help menu with 'zig build -h'\n", .{});
        process.exit(1);
    }
}

/// Starting from all top-level steps in `b`, traverses the entire step graph
/// and adds all step dependencies implied by module graphs.
fn createModuleDependencies(b: *std.Build) Allocator.Error!void {
    const arena = b.graph.arena;

    var all_steps: std.AutoArrayHashMapUnmanaged(*Step, void) = .empty;
    var next_step_idx: usize = 0;

    try all_steps.ensureUnusedCapacity(arena, b.top_level_steps.count());
    for (b.top_level_steps.values()) |tls| {
        all_steps.putAssumeCapacityNoClobber(&tls.step, {});
    }

    while (next_step_idx < all_steps.count()) {
        const step = all_steps.keys()[next_step_idx];
        next_step_idx += 1;

        // Set up any implied dependencies for this step. It's important that we do this first, so
        // that the loop below discovers steps implied by the module graph.
        try createModuleDependenciesForStep(step);

        try all_steps.ensureUnusedCapacity(arena, step.dependencies.items.len);
        for (step.dependencies.items) |other_step| {
            all_steps.putAssumeCapacity(other_step, {});
        }
    }
}

/// If the given `Step` is a `Step.Compile`, adds any dependencies for that step which
/// are implied by the module graph rooted at `step.cast(Step.Compile).?.root_module`.
fn createModuleDependenciesForStep(step: *Step) Allocator.Error!void {
    const root_module = if (step.cast(Step.Compile)) |cs| root: {
        break :root cs.root_module;
    } else return; // not a compile step so no module dependencies

    // Starting from `root_module`, discover all modules in this graph.
    const modules = root_module.getGraph().modules;

    // For each of those modules, set up the implied step dependencies.
    for (modules) |mod| {
        if (mod.root_source_file) |lp| lp.addStepDependencies(step);
        for (mod.include_dirs.items) |include_dir| switch (include_dir) {
            .path,
            .path_system,
            .path_after,
            .framework_path,
            .framework_path_system,
            .embed_path,
            => |lp| lp.addStepDependencies(step),

            .other_step => |other| {
                other.getEmittedIncludeTree().addStepDependencies(step);
                step.dependOn(&other.step);
            },

            .config_header_step => |other| step.dependOn(&other.step),
        };
        for (mod.lib_paths.items) |lp| lp.addStepDependencies(step);
        for (mod.rpaths.items) |rpath| switch (rpath) {
            .lazy_path => |lp| lp.addStepDependencies(step),
            .special => {},
        };
        for (mod.link_objects.items) |link_object| switch (link_object) {
            .static_path,
            .assembly_file,
            => |lp| lp.addStepDependencies(step),
            .other_step => |other| step.dependOn(&other.step),
            .system_lib => {},
            .c_source_file => |source| source.file.addStepDependencies(step),
            .c_source_files => |source_files| source_files.root.addStepDependencies(step),
            .win32_resource_file => |rc_source| {
                rc_source.file.addStepDependencies(step);
                for (rc_source.include_paths) |lp| lp.addStepDependencies(step);
            },
        };
    }
}

//
//
// ZLS code
//
//

const shared = @import("shared.zig");
const Transport = shared.Transport;
const BuildConfig = shared.BuildConfig;

fn extractBuildInformation(
    b: *std.Build,
    arena: Allocator,
    main_progress_node: std.Progress.Node,
    run: *Run,
) !void {
    const helper = struct {
        fn addLazyPathStepDependencies(allocator: Allocator, set: *std.AutoArrayHashMapUnmanaged(*Step, void), lazy_path: std.Build.LazyPath) !void {
            switch (lazy_path) {
                .src_path, .cwd_relative, .dependency => {},
                .generated => |gen| try set.put(allocator, gen.file.step, {}),
            }
        }
        fn addIncludeDirStepDependencies(allocator: Allocator, set: *std.AutoArrayHashMapUnmanaged(*Step, void), include_dir: std.Build.Module.IncludeDir) !void {
            switch (include_dir) {
                .path,
                .path_system,
                .path_after,
                .framework_path,
                .framework_path_system,
                => |lazy_path| try addLazyPathStepDependencies(allocator, set, lazy_path),
                .other_step => |other| {
                    if (other.generated_h) |header| {
                        try set.put(allocator, header.step, {});
                    }
                    if (other.installed_headers_include_tree) |include_tree| {
                        try set.put(allocator, include_tree.generated_directory.step, {});
                    }
                },
                .embed_path => {
                    // This only affects C source files
                },
                .config_header_step => |config_header| try set.put(allocator, &config_header.step, {}),
            }
        }
        /// Only adds the necessary dependencies to resolve the `root_source_file` and `include_dirs`. Does not include dependencies of imported modules.
        fn addModuleDependencies(allocator: Allocator, set: *std.AutoArrayHashMapUnmanaged(*Step, void), module: *std.Build.Module) !void {
            if (module.root_source_file) |root_source_file| {
                try addLazyPathStepDependencies(allocator, set, root_source_file);
            }

            for (module.include_dirs.items) |include_dir| {
                try addIncludeDirStepDependencies(allocator, set, include_dir);
            }
        }
        fn processModule(
            allocator: Allocator,
            modules: *std.StringArrayHashMapUnmanaged(shared.BuildConfig.Module),
            module: *std.Build.Module,
            compile: ?*Step.Compile,
        ) !void {
            const root_source_file = module.root_source_file orelse return;

            var include_dirs: std.StringArrayHashMapUnmanaged(void) = .empty;
            var c_macros: std.StringArrayHashMapUnmanaged(void) = .empty;

            if (compile) |exe| {
                try processPkgConfig(allocator, &include_dirs, &c_macros, exe);
            }

            try c_macros.ensureUnusedCapacity(allocator, module.c_macros.items.len);
            for (module.c_macros.items) |c_macro| {
                c_macros.putAssumeCapacity(c_macro, {});
            }

            for (module.include_dirs.items) |include_dir| {
                switch (include_dir) {
                    .path,
                    .path_system,
                    .path_after,
                    .framework_path,
                    .framework_path_system,
                    => |include_path| try include_dirs.put(allocator, include_path.getPath(module.owner), {}),

                    .other_step => |other| {
                        if (other.generated_h) |header| {
                            try include_dirs.put(
                                allocator,
                                std.fs.path.dirname(header.getPath()).?,
                                {},
                            );
                        }
                        if (other.installed_headers_include_tree) |include_tree| {
                            try include_dirs.put(
                                allocator,
                                include_tree.generated_directory.getPath(),
                                {},
                            );
                        }
                    },
                    .embed_path => {
                        // This only affects C source files
                    },
                    .config_header_step => |config_header| {
                        try include_dirs.put(
                            allocator,
                            config_header.generated_dir.getPath(),
                            {},
                        );
                    },
                }
            }

            const cwd = module.owner.graph.cache.cwd;

            const root_source_file_path = try std.fs.path.resolve(allocator, &.{ cwd, root_source_file.getPath2(module.owner, null) });

            // All modules with the same root source file are merged. This limitation may be lifted in the future.
            const gop = try modules.getOrPutValue(allocator, root_source_file_path, .{
                .import_table = .{},
                .c_macros = &.{},
                .include_dirs = &.{},
            });

            for (module.import_table.keys(), module.import_table.values()) |name, import| {
                const gop_import = try gop.value_ptr.import_table.map.getOrPut(allocator, name);
                // This does not account for the possibility of collisions (i.e. modules with same root source file import different modules under the same name).
                if (!gop_import.found_existing) {
                    gop_import.value_ptr.* = try std.fs.path.resolve(allocator, &.{ cwd, import.root_source_file.?.getPath2(import.owner, null) });
                }
            }
            gop.value_ptr.c_macros = try std.mem.concat(allocator, []const u8, &.{ gop.value_ptr.c_macros, c_macros.keys() });
            gop.value_ptr.include_dirs = try std.mem.concat(allocator, []const u8, &.{ gop.value_ptr.include_dirs, include_dirs.keys() });
        }
    };
    const gpa = run.gpa;

    // The value tracks whether the step is a decendant of the "install" step.
    var all_steps: std.AutoArrayHashMapUnmanaged(*Step, bool) = .empty;
    defer all_steps.deinit(gpa);

    // collect all steps that are decendants of the "install" step.
    {
        try all_steps.putNoClobber(gpa, b.getInstallStep(), true);

        var i: usize = 0;
        while (i < all_steps.count()) : (i += 1) {
            const step = all_steps.keys()[i];

            try all_steps.ensureUnusedCapacity(gpa, step.dependencies.items.len);
            for (step.dependencies.items) |other_step| {
                all_steps.putAssumeCapacity(other_step, true);
            }
        }
    }

    // collect all other steps
    {
        var i: usize = all_steps.count();

        try all_steps.ensureUnusedCapacity(gpa, b.top_level_steps.count());
        for (b.top_level_steps.values()) |tls| {
            all_steps.putAssumeCapacity(&tls.step, true);
        }

        while (i < all_steps.count()) : (i += 1) {
            const step = all_steps.keys()[i];

            try all_steps.ensureUnusedCapacity(gpa, step.dependencies.items.len);
            for (step.dependencies.items) |other_step| {
                all_steps.putAssumeCapacity(other_step, false);
            }
        }
    }

    // Collect all steps that need to be run so that we can resolve the lazy paths we are interested in (e.g. root_source_file).
    {
        var needed_steps: std.AutoArrayHashMapUnmanaged(*Step, void) = .empty;
        defer needed_steps.deinit(gpa);

        var modules: std.AutoArrayHashMapUnmanaged(*std.Build.Module, void) = .empty;
        defer modules.deinit(gpa);

        try modules.ensureUnusedCapacity(gpa, b.modules.count());
        for (b.modules.values()) |root_module| {
            modules.putAssumeCapacity(root_module, {});
        }

        // collect all modules of `Step.Compile`
        for (all_steps.keys()) |step| {
            const compile = step.cast(Step.Compile) orelse continue;
            const graph = compile.root_module.getGraph();
            try modules.ensureUnusedCapacity(gpa, graph.modules.len);
            for (graph.modules) |module| modules.putAssumeCapacity(module, {});
        }

        // collect all dependencies of all found modules
        for (modules.keys()) |module| {
            try helper.addModuleDependencies(gpa, &needed_steps, module);
        }

        prepare(b, &needed_steps, run) catch |err| switch (err) {
            error.DependencyLoopDetected => process.exit(1),
            else => |e| return e,
        };

        try runSteps(
            b,
            &needed_steps,
            main_progress_node,
            run,
        );
    }

    // We collect modules in the following order:
    // - public modules (`std.Build.addModule`)
    // - modules that are reachable from the "install" step
    // - all other reachable modules
    var modules: std.StringArrayHashMapUnmanaged(BuildConfig.Module) = .empty;

    for (b.modules.values()) |root_module| {
        const graph = root_module.getGraph();
        for (graph.modules) |module| {
            try helper.processModule(arena, &modules, module, null);
        }
    }

    // We loop twice through all steps so that decendants of the "install" step are processed first.
    for ([_]bool{ true, false }) |want_install_step_decendant| {
        for (all_steps.keys(), all_steps.values()) |step, is_install_step_decendant| {
            if (is_install_step_decendant != want_install_step_decendant) continue;

            const compile = step.cast(Step.Compile) orelse continue;
            const graph = compile.root_module.getGraph();
            for (graph.modules) |module| {
                try helper.processModule(arena, &modules, module, compile);
            }
        }
    }

    var compilations: std.ArrayList(BuildConfig.Compile) = .empty;
    for (all_steps.keys()) |step| {
        const compile = step.cast(Step.Compile) orelse continue;
        const root_source_file = compile.root_module.root_source_file orelse continue;
        const root_source_file_path = try std.fs.path.resolve(arena, &.{ b.graph.cache.cwd, root_source_file.getPath2(compile.root_module.owner, null) });
        try compilations.append(arena, .{
            .root_module = root_source_file_path,
        });
    }

    // Sample `@dependencies` structure:
    // pub const packages = struct {
    //     pub const @"1220363c7e27b2d3f39de6ff6e90f9537a0634199860fea237a55ddb1e1717f5d6a5" = struct {
    //         pub const build_root = "/home/rad/.cache/zig/p/1220363c7e27b2d3f39de6ff6e90f9537a0634199860fea237a55ddb1e1717f5d6a5";
    //         pub const build_zig = @import("1220363c7e27b2d3f39de6ff6e90f9537a0634199860fea237a55ddb1e1717f5d6a5");
    //         pub const deps: []const struct { []const u8, []const u8 } = &.{};
    //     };
    // ...
    // };
    // pub const root_deps: []const struct { []const u8, []const u8 } = &.{
    //     .{ "known_folders", "1220bb12c9bfe291eed1afe6a2070c7c39918ab1979f24a281bba39dfb23f5bcd544" },
    //     .{ "diffz", "122089a8247a693cad53beb161bde6c30f71376cd4298798d45b32740c3581405864" },
    // };

    // Collect the dependencies from `build.zig.zon`
    var root_dependencies: std.StringArrayHashMapUnmanaged([]const u8) = .empty;
    for (dependencies.root_deps) |root_dep| {
        inline for (comptime std.meta.declarations(dependencies.packages)) |package| blk: {
            if (std.mem.eql(u8, package.name, root_dep[1])) {
                const package_info = @field(dependencies.packages, package.name);
                if (!@hasDecl(package_info, "build_root")) break :blk;
                if (!@hasDecl(package_info, "build_zig")) break :blk;
                try root_dependencies.put(
                    arena,
                    root_dep[0],
                    try std.fs.path.join(arena, &.{ package_info.build_root, "build.zig" }),
                );
            }
        }
    }

    var available_options: std.StringArrayHashMapUnmanaged(BuildConfig.AvailableOption) = .empty;
    try available_options.ensureTotalCapacity(arena, b.available_options_map.count());

    var it = b.available_options_map.iterator();
    while (it.next()) |available_option| {
        available_options.putAssumeCapacityNoClobber(available_option.key_ptr.*, available_option.value_ptr.*);
    }

    const stringified_build_config = try std.json.Stringify.valueAlloc(
        gpa,
        BuildConfig{
            .dependencies = .{ .map = root_dependencies },
            .modules = .{ .map = modules },
            .compilations = compilations.items,
            .top_level_steps = b.top_level_steps.keys(),
            .available_options = .{ .map = available_options },
        },
        .{ .whitespace = .indent_2 },
    );

    var file_writer = Io.File.stdout().writer(b.graph.io, &.{});
    file_writer.interface.writeAll(stringified_build_config) catch return file_writer.err.?;
}

fn processPkgConfig(
    allocator: Allocator,
    include_dirs: *std.StringArrayHashMapUnmanaged(void),
    c_macros: *std.StringArrayHashMapUnmanaged(void),
    exe: *Step.Compile,
) !void {
    for (exe.root_module.link_objects.items) |link_object| {
        if (link_object != .system_lib) continue;
        const system_lib = link_object.system_lib;

        if (system_lib.use_pkg_config == .no) continue;

        const args = copied_from_zig.runPkgConfig(exe, system_lib.name) catch |err| switch (err) {
            error.PkgConfigInvalidOutput,
            error.PkgConfigCrashed,
            error.PkgConfigFailed,
            error.PkgConfigNotInstalled,
            error.PackageNotFound,
            => switch (system_lib.use_pkg_config) {
                .yes => {
                    // pkg-config failed, so zig will not add any include paths
                    continue;
                },
                .force => {
                    std.log.warn("pkg-config failed for library {s}", .{system_lib.name});
                    continue;
                },
                .no => unreachable,
            },
            else => |e| return e,
        };
        for (args) |arg| {
            if (std.mem.startsWith(u8, arg, "-I")) {
                const candidate = arg[2..];
                try include_dirs.put(allocator, candidate, {});
            } else if (std.mem.startsWith(u8, arg, "-D")) {
                try c_macros.put(allocator, arg, {});
            }
        }
    }
}

// TODO: Having a copy of this is not very nice
const copied_from_zig = struct {
    /// Run pkg-config for the given library name and parse the output, returning the arguments
    /// that should be passed to zig to link the given library.
    fn runPkgConfig(self: *Step.Compile, lib_name: []const u8) ![]const []const u8 {
        const b = self.step.owner;
        const pkg_name = match: {
            // First we have to map the library name to pkg config name. Unfortunately,
            // there are several examples where this is not straightforward:
            // -lSDL2 -> pkg-config sdl2
            // -lgdk-3 -> pkg-config gdk-3.0
            // -latk-1.0 -> pkg-config atk
            const pkgs = try getPkgConfigList(b);

            // Exact match means instant winner.
            for (pkgs) |pkg| {
                if (mem.eql(u8, pkg.name, lib_name)) {
                    break :match pkg.name;
                }
            }

            // Next we'll try ignoring case.
            for (pkgs) |pkg| {
                if (std.ascii.eqlIgnoreCase(pkg.name, lib_name)) {
                    break :match pkg.name;
                }
            }

            // Now try appending ".0".
            for (pkgs) |pkg| {
                if (std.ascii.indexOfIgnoreCase(pkg.name, lib_name)) |pos| {
                    if (pos != 0) continue;
                    if (mem.eql(u8, pkg.name[lib_name.len..], ".0")) {
                        break :match pkg.name;
                    }
                }
            }

            // Trimming "-1.0".
            if (mem.endsWith(u8, lib_name, "-1.0")) {
                const trimmed_lib_name = lib_name[0 .. lib_name.len - "-1.0".len];
                for (pkgs) |pkg| {
                    if (std.ascii.eqlIgnoreCase(pkg.name, trimmed_lib_name)) {
                        break :match pkg.name;
                    }
                }
            }

            return error.PackageNotFound;
        };

        var code: u8 = undefined;
        const stdout = if (b.runAllowFail(&.{
            "pkg-config",
            pkg_name,
            "--cflags",
            "--libs",
        }, &code, .ignore)) |stdout| stdout else |err| switch (err) {
            error.ProcessTerminated => return error.PkgConfigCrashed,
            error.ExecNotSupported => return error.PkgConfigFailed,
            error.ExitCodeFailure => return error.PkgConfigFailed,
            error.FileNotFound => return error.PkgConfigNotInstalled,
            else => return err,
        };

        var zig_args = std.array_list.Managed([]const u8).init(b.allocator);
        defer zig_args.deinit();

        var it = mem.tokenizeAny(u8, stdout, " \r\n\t");
        while (it.next()) |tok| {
            if (mem.eql(u8, tok, "-I")) {
                const dir = it.next() orelse return error.PkgConfigInvalidOutput;
                try zig_args.appendSlice(&.{ "-I", dir });
            } else if (mem.startsWith(u8, tok, "-I")) {
                try zig_args.append(tok);
            } else if (mem.eql(u8, tok, "-L")) {
                const dir = it.next() orelse return error.PkgConfigInvalidOutput;
                try zig_args.appendSlice(&.{ "-L", dir });
            } else if (mem.startsWith(u8, tok, "-L")) {
                try zig_args.append(tok);
            } else if (mem.eql(u8, tok, "-l")) {
                const lib = it.next() orelse return error.PkgConfigInvalidOutput;
                try zig_args.appendSlice(&.{ "-l", lib });
            } else if (mem.startsWith(u8, tok, "-l")) {
                try zig_args.append(tok);
            } else if (mem.eql(u8, tok, "-D")) {
                const macro = it.next() orelse return error.PkgConfigInvalidOutput;
                try zig_args.appendSlice(&.{ "-D", macro });
            } else if (mem.startsWith(u8, tok, "-D")) {
                try zig_args.append(tok);
            } else if (b.debug_pkg_config) {
                return self.step.fail("unknown pkg-config flag '{s}'", .{tok});
            }
        }

        return zig_args.toOwnedSlice();
    }

    fn execPkgConfigList(self: *std.Build, out_code: *u8) (std.Build.PkgConfigError || std.Build.RunError)![]const std.Build.PkgConfigPkg {
        const stdout = try self.runAllowFail(&.{ "pkg-config", "--list-all" }, out_code, .ignore);
        var list = std.array_list.Managed(std.Build.PkgConfigPkg).init(self.allocator);
        errdefer list.deinit();
        var line_it = mem.tokenizeAny(u8, stdout, "\r\n");
        while (line_it.next()) |line| {
            if (mem.trim(u8, line, " \t").len == 0) continue;
            var tok_it = mem.tokenizeAny(u8, line, " \t");
            try list.append(.{
                .name = tok_it.next() orelse return error.PkgConfigInvalidOutput,
                .desc = tok_it.rest(),
            });
        }
        return list.toOwnedSlice();
    }

    fn getPkgConfigList(self: *std.Build) ![]const std.Build.PkgConfigPkg {
        if (self.pkg_config_pkg_list) |res| {
            return res;
        }
        var code: u8 = undefined;
        if (execPkgConfigList(self, &code)) |list| {
            self.pkg_config_pkg_list = list;
            return list;
        } else |err| {
            const result = switch (err) {
                error.ProcessTerminated => error.PkgConfigCrashed,
                error.ExecNotSupported => error.PkgConfigFailed,
                error.ExitCodeFailure => error.PkgConfigFailed,
                error.FileNotFound => error.PkgConfigNotInstalled,
                error.InvalidName => error.PkgConfigNotInstalled,
                error.PkgConfigInvalidOutput => error.PkgConfigInvalidOutput,
                else => return err,
            };
            self.pkg_config_pkg_list = result;
            return result;
        }
    }
};

fn serveWatchErrorBundle(
    io: std.Io,
    step_id: u32,
    cycle: u32,
    error_bundle: std.zig.ErrorBundle,
) Io.File.Writer.Error!void {
    const bytes_len = @sizeOf(shared.ServerToClient.ErrorBundle) + @sizeOf(u32) * error_bundle.extra.len + error_bundle.string_bytes.len;

    var header: shared.ServerToClient.Header = .{
        .tag = .watch_error_bundle,
        .bytes_len = @intCast(bytes_len),
    };

    var error_bundle_header: shared.ServerToClient.ErrorBundle = .{
        .step_id = step_id,
        .cycle = cycle,
        .extra_len = @intCast(error_bundle.extra.len),
        .string_bytes_len = @intCast(error_bundle.string_bytes.len),
    };

    const need_bswap = builtin.target.cpu.arch.endian() != .little;

    if (need_bswap) {
        std.mem.byteSwapAllFields(shared.ServerToClient.Header, &header);
        std.mem.byteSwapAllFields(shared.ServerToClient.ErrorBundle, &error_bundle_header);
        std.mem.byteSwapAllElements(u32, @constCast(error_bundle.extra)); // trust me bro
    }

    var file_writer = Io.File.stdout().writer(io, &.{});
    const writer = &file_writer.interface;

    var data = [_][]const u8{
        std.mem.asBytes(&header),
        std.mem.asBytes(&error_bundle_header),
        std.mem.sliceAsBytes(error_bundle.extra),
        error_bundle.string_bytes,
    };
    writer.writeVecAll(&data) catch return file_writer.err.?;
}
